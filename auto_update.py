#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Auto Update Stata Journal Database with Citation Information
è‡ªåŠ¨æ›´æ–° Stata Journal æ•°æ®åº“ï¼ŒåŒ…å«å®Œæ•´å¼•æ–‡ä¿¡æ¯

åŠŸèƒ½ï¼š
1. ä» Stata Journal å®˜ç½‘çˆ¬å–æ–‡ç« åˆ—è¡¨å’Œ DOI
2. é€šè¿‡ CrossRef API è·å–è¯¦ç»†å¼•æ–‡ä¿¡æ¯ï¼ˆä½œè€…ã€æ ‡é¢˜ã€æ‘˜è¦ã€å¼•ç”¨æ¬¡æ•°ç­‰ï¼‰
3. æ›´æ–°æœ¬åœ°æ•°æ®åº“æ–‡ä»¶

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2026-02-03
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from datetime import datetime
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åŸºç¡€é…ç½®
BASE_URL = "https://www.stata-journal.com"
CROSSREF_API = "https://api.crossref.org/works"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
}

def get_crossref_citation(doi, retry=3):
    """
    ä» CrossRef API è·å–è¯¦ç»†å¼•æ–‡ä¿¡æ¯
    
    Args:
        doi: DOI æ ‡è¯†ç¬¦
        retry: é‡è¯•æ¬¡æ•°
    
    Returns:
        dict: åŒ…å«å¼•æ–‡ä¿¡æ¯çš„å­—å…¸
    """
    if not doi:
        return {}
    
    url = f"{CROSSREF_API}/{doi}"
    
    for attempt in range(retry):
        try:
            time.sleep(0.2)  # åœ¨æ¯æ¬¡è¯·æ±‚å‰æ·»åŠ å»¶è¿Ÿ
            response = requests.get(url, timeout=15)
            
            # ç‰¹æ®Šå¤„ç†429é”™è¯¯
            if response.status_code == 429:
                wait_time = 60  # ç­‰å¾…60ç§’
                logger.warning(f"Rate limit hit for DOI {doi}. Waiting {wait_time} seconds...")
                time.sleep(wait_time)
                continue
            
            response.raise_for_status()
            data = response.json()
            
            if 'message' not in data:
                return {}
            
            message = data['message']
            
            # æå–å…³é”®ä¿¡æ¯
            citation_info = {
                'doi': message.get('DOI', doi),
                'title': message.get('title', [''])[0] if message.get('title') else '',
                'container_title': message.get('container-title', [''])[0] if message.get('container-title') else '',
                'publisher': message.get('publisher', ''),
                'volume': str(message.get('volume', '')),
                'issue': str(message.get('issue', '')),
                'page': message.get('page', ''),
                'article_type': message.get('type', ''),
                'reference_count': message.get('reference-count', 0),
                'cited_by_count': message.get('is-referenced-by-count', 0),
                'url': message.get('URL', ''),
            }
            
            # æå–å‡ºç‰ˆæ—¥æœŸ
            published = message.get('published-print') or message.get('published-online') or {}
            date_parts = published.get('date-parts', [[]])
            if date_parts and date_parts[0]:
                parts = date_parts[0]
                citation_info['year'] = parts[0] if len(parts) > 0 else ''
                citation_info['month'] = parts[1] if len(parts) > 1 else ''
            else:
                citation_info['year'] = ''
                citation_info['month'] = ''
            
            # æå–ä½œè€…ä¿¡æ¯
            authors = message.get('author', [])
            if authors:
                # ç¬¬ä¸€ä½œè€…
                first_author = authors[0]
                citation_info['first_author_family'] = first_author.get('family', '')
                citation_info['first_author_given'] = first_author.get('given', '')
                
                # æ‰€æœ‰ä½œè€…ï¼ˆæ ¼å¼åŒ–ï¼‰
                author_list = []
                for author in authors:
                    family = author.get('family', '')
                    given = author.get('given', '')
                    if family:
                        author_list.append(f"{family}, {given}" if given else family)
                
                citation_info['authors'] = '; '.join(author_list)
                citation_info['author_count'] = len(authors)
            else:
                citation_info['first_author_family'] = ''
                citation_info['first_author_given'] = ''
                citation_info['authors'] = ''
                citation_info['author_count'] = 0
            
            # æå–æ‘˜è¦ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
            abstract = message.get('abstract', '')
            if abstract:
                # å»é™¤ HTML/XML æ ‡ç­¾
                clean_abstract = re.sub(r'<[^>]+>', '', abstract)
                citation_info['abstract'] = clean_abstract.strip()
            else:
                citation_info['abstract'] = ''
            
            # æå– PDF é“¾æ¥
            links = message.get('link', [])
            pdf_url = ''
            for link in links:
                if 'pdf' in link.get('content-type', '').lower():
                    pdf_url = link.get('URL', '')
                    break
            citation_info['pdf_url'] = pdf_url
            
            # ISSN
            issn_list = message.get('ISSN', [])
            citation_info['issn'] = issn_list[0] if issn_list else ''
            
            # æ ¼å¼åŒ–çš„å¼•ç”¨æ–‡æœ¬ (APA style)
            citation_text = format_citation_apa(citation_info)
            citation_info['citation_apa'] = citation_text
            
            logger.debug(f"Successfully fetched citation for DOI: {doi}")
            return citation_info
            
        except requests.RequestException as e:
            logger.warning(f"Attempt {attempt + 1}/{retry} failed for DOI {doi}: {e}")
            if attempt < retry - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        except Exception as e:
            logger.error(f"Error parsing citation for DOI {doi}: {e}")
            return {}
    
    logger.error(f"Failed to fetch citation for DOI: {doi}")
    return {}

def format_citation_apa(info):
    """
    æ ¼å¼åŒ–å¼•ç”¨æ–‡æœ¬ï¼ˆAPA é£æ ¼ï¼‰
    
    Args:
        info: å¼•æ–‡ä¿¡æ¯å­—å…¸
    
    Returns:
        str: æ ¼å¼åŒ–çš„å¼•ç”¨æ–‡æœ¬
    """
    parts = []
    
    # ä½œè€…
    if info.get('authors'):
        parts.append(info['authors'])
    
    # å¹´ä»½
    if info.get('year'):
        parts.append(f"({info['year']})")
    
    # æ ‡é¢˜
    if info.get('title'):
        parts.append(info['title'] + '.')
    
    # æœŸåˆŠ
    if info.get('container_title'):
        journal_part = info['container_title']
        
        # å·æœŸé¡µç 
        if info.get('volume'):
            journal_part += f", {info['volume']}"
            if info.get('issue'):
                journal_part += f"({info['issue']})"
        
        if info.get('page'):
            journal_part += f": {info['page']}"
        
        parts.append(journal_part + '.')
    
    return ' '.join(parts)

def get_web_info(doi, vol, num, title_fallback=''):
    """
    ä»ç½‘é¡µè·å–æ–‡ç« çš„å®Œæ•´ä¿¡æ¯ï¼ˆä½œè€…ã€æ ‡é¢˜ã€æ‘˜è¦ã€é¡µç ç­‰ï¼‰
    
    Args:
        doi: DOI æ ‡è¯†ç¬¦
        vol: å·å·
        num: æœŸå·
        title_fallback: å¤‡ç”¨æ ‡é¢˜
    
    Returns:
        dict: åŒ…å«æ–‡ç« ä¿¡æ¯çš„å­—å…¸
    """
    if not doi:
        return {}
    
    # æ„é€ æ–‡ç« é¡µé¢ URLï¼ˆé€šè¿‡DOIï¼‰
    article_url = f"https://journals.sagepub.com/doi/{doi}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(article_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        result = {}
        
        # æå–ä½œè€…ä¿¡æ¯
        # æŸ¥æ‰¾ä½œè€…åˆ—è¡¨
        author_list = []
        author_section = soup.find('div', class_='accordion-tabbed loa-accordion')
        if not author_section:
            # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
            author_section = soup.find('div', class_='author-list')
        
        if author_section:
            # æŸ¥æ‰¾æ‰€æœ‰ä½œè€…é“¾æ¥æˆ–span
            author_elements = author_section.find_all(['a', 'span'], class_=re.compile(r'author'))
            for elem in author_elements:
                author_name = elem.get_text(strip=True)
                if author_name and len(author_name) > 1:
                    author_list.append(author_name)
        
        if author_list:
            result['authors'] = '; '.join(author_list)
            # æå–ç¬¬ä¸€ä½œè€…
            first_author = author_list[0]
            # å°è¯•åˆ†ç¦»å§“å’Œå
            if ',' in first_author:
                parts = first_author.split(',', 1)
                result['first_author_family'] = parts[0].strip()
                result['first_author_given'] = parts[1].strip() if len(parts) > 1 else ''
            else:
                # å‡è®¾æœ€åä¸€ä¸ªå•è¯æ˜¯å§“
                parts = first_author.split()
                if len(parts) > 1:
                    result['first_author_family'] = parts[-1]
                    result['first_author_given'] = ' '.join(parts[:-1])
                else:
                    result['first_author_family'] = first_author
                    result['first_author_given'] = ''
            result['author_count'] = len(author_list)
        
        # æå–æ‘˜è¦
        abstract_section = soup.find('div', class_='abstractSection')
        if not abstract_section:
            abstract_section = soup.find('section', {'data-wrapper': 'abstract'})
        if not abstract_section:
            abstract_section = soup.find('div', class_='article-section__content', attrs={'role': 'paragraph'})
        
        if abstract_section:
            # è·å–çº¯æ–‡æœ¬ï¼Œå»é™¤HTMLæ ‡ç­¾
            abstract_text = abstract_section.get_text(separator=' ', strip=True)
            # æ¸…ç†å¤šä½™ç©ºæ ¼
            abstract_text = re.sub(r'\s+', ' ', abstract_text)
            result['abstract'] = abstract_text
        else:
            result['abstract'] = ''
        
        # æå–æ ‡é¢˜
        title_elem = soup.find('h1', class_='citation__title')
        if not title_elem:
            title_elem = soup.find('h1', class_='article-title')
        if title_elem:
            result['title'] = title_elem.get_text(strip=True)
        else:
            result['title'] = title_fallback
        
        # æå–é¡µç 
        page_elem = soup.find('span', class_='article-page-range')
        if not page_elem:
            page_elem = soup.find('span', class_='page-range')
        if page_elem:
            result['page'] = page_elem.get_text(strip=True)
        else:
            result['page'] = ''
        
        # æå–PDFé“¾æ¥
        pdf_link = soup.find('a', class_='show-pdf')
        if not pdf_link:
            pdf_link = soup.find('a', href=re.compile(r'/doi/pdf/'))
        if pdf_link:
            pdf_href = pdf_link.get('href', '')
            if pdf_href.startswith('/'):
                result['pdf_url'] = f"https://journals.sagepub.com{pdf_href}"
            else:
                result['pdf_url'] = pdf_href
        else:
            result['pdf_url'] = ''
        
        # URL
        result['url'] = article_url
        
        if result:
            logger.debug(f"Web info for DOI {doi}: {list(result.keys())}")
        
        return result
        
    except Exception as e:
        logger.debug(f"Failed to get web info for DOI {doi}: {e}")
        return {}

def get_article_doi_from_page(artid):
    """
    ä»æ–‡ç« é¡µé¢è·å– DOI
    
    Args:
        artid: æ–‡ç« IDï¼ˆå¦‚ st0001, dm122ï¼‰
    
    Returns:
        str: DOI æˆ–ç©ºå­—ç¬¦ä¸²
    """
    url = f"{BASE_URL}/article.html?article={artid}"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾ Sage æœŸåˆŠé“¾æ¥
        sage_link = soup.find('a', href=re.compile(r'journals\.sagepub\.com/doi/'))
        if sage_link:
            href = sage_link.get('href', '')
            doi_match = re.search(r'doi/(pdf/)?(10\.\d+/[^?&#]+)', href)
            if doi_match:
                return doi_match.group(2).lower()
        
        # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ DOI é“¾æ¥
        doi_link = soup.find('a', href=re.compile(r'doi\.org/'))
        if doi_link:
            href = doi_link.get('href', '')
            doi_match = re.search(r'10\.\d+/[^?&#\s]+', href)
            if doi_match:
                return doi_match.group(0).lower()
        
        return ''
    except Exception as e:
        logger.debug(f"Failed to get DOI for {artid}: {e}")
        return ''

def get_all_stata_journal_articles():
    """
    ä» Stata Journal æœç´¢é¡µé¢è·å–æ‰€æœ‰æ–‡ç« çš„ artid å’ŒåŸºç¡€ä¿¡æ¯
    
    Returns:
        list: æ–‡ç« ä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å« artid, title, volume, number, yearï¼‰
    """
    logger.info("Fetching all articles from search page...")
    
    search_url = f"{BASE_URL}/sjsearch.html?choice=keyword&q="
    
    try:
        response = requests.get(search_url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰ article.html?article= é“¾æ¥
        article_links = soup.find_all('a', href=re.compile(r'article\.html\?article='))
        
        all_articles = []
        seen_artids = set()
        skipped_count = 0
        
        for link in article_links:
            href = link.get('href', '')
            match = re.search(r'article=([^"&]+)', href)
            if match:
                artid = match.group(1).strip()
                
                # è·³è¿‡ updates, announcements, emptytag ç­‰éæ­£å¼æ–‡ç« 
                artid_lower = artid.lower()
                if artid_lower.startswith(('up', 'an', 'emptytag')):
                    skipped_count += 1
                    continue
                
                # å»é‡
                if artid in seen_artids:
                    continue
                seen_artids.add(artid)
                
                # è·å–æ ‡é¢˜
                title = link.get_text(strip=True)
                
                # è·³è¿‡ Software updates
                if 'software update' in title.lower():
                    skipped_count += 1
                    continue
                
                # ä» artid æ¨æ–­å·æœŸå¹´ä»½ï¼ˆæ ¼å¼å¦‚ st0001, dm122ï¼‰
                # artidæ ¼å¼: {type}{number}, ä¾‹å¦‚ st1234
                artid_match = re.match(r'([a-z]+)(\d+)', artid)
                if artid_match:
                    article_type = artid_match.group(1)
                    article_num = int(artid_match.group(2))
                    
                    # ç²—ç•¥ä¼°ç®—ï¼šå‡è®¾stæ–‡ç« æ¯å¹´60ç¯‡å·¦å³
                    # å®é™…ä¼šåœ¨CrossRef APIä¸­è·å–å‡†ç¡®çš„å·æœŸä¿¡æ¯
                    vol = 0
                    num = 0
                    year = 0
                    
                    all_articles.append({
                        'artid': artid,
                        'title_web': title,
                        'volume': vol,  # å¾…è¡¥å……
                        'number': num,  # å¾…è¡¥å……
                        'year': year    # å¾…è¡¥å……
                    })
        
        logger.info(f"Found {len(all_articles)} articles (skipped {skipped_count} non-article entries)")
        return all_articles
        
    except Exception as e:
        logger.error(f"Error fetching from search page: {e}")
        return []

def update_database():
    """
    æ›´æ–°æ•°æ®åº“ä¸»å‡½æ•°
    """
    logger.info("=" * 60)
    logger.info("Starting Stata Journal Database Update")
    logger.info("=" * 60)
    
    # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰æ–‡ç« åˆ—è¡¨
    logger.info("\nStep 1: Fetching article list from Stata Journal website...")
    articles = get_all_stata_journal_articles()
    
    if not articles:
        logger.error("No articles found! Exiting.")
        return
    
    # ç¬¬äºŒæ­¥ï¼šè·å–å¼•æ–‡ä¿¡æ¯
    logger.info(f"\nStep 2: Fetching citation information from CrossRef API...")
    logger.info(f"Total articles to process: {len(articles)}")
    
    detailed_articles = []
    
    def process_article(art, index):
        """å¤„ç†å•ç¯‡æ–‡ç« """
        artid = art.get('artid', '')
        vol = art.get('volume', '')
        num = art.get('number', '')
        year = art.get('year', '')
        
        # å…ˆè·å–DOI
        doi = get_article_doi_from_page(artid)
        
        if not doi:
            # æ²¡æœ‰ DOI çš„æ–‡ç« ï¼Œåªä¿ç•™åŸºæœ¬ä¿¡æ¯
            result = {
                'art_id': artid,
                'title': art.get('title_web', ''),
                'volume': vol,
                'number': num,
                'year': year,
                'doi': '',
                'authors': '',
                'first_author_family': '',
                'first_author_given': '',
                'author_count': 0,
                'abstract': '',
                'page': '',
                'reference_count': 0,
                'cited_by_count': 0,
                'citation_apa': '',
                'url': f'{BASE_URL}/article.html?article={artid}',
                'pdf_url': '',
            }
        else:
            # ä» CrossRef è·å–è¯¦ç»†ä¿¡æ¯
            citation = get_crossref_citation(doi)
            
            if citation:
                result = {
                    'art_id': artid,
                    'title': citation.get('title', art.get('title_web', '')),
                    'volume': citation.get('volume', vol),
                    'number': citation.get('issue', num),
                    'year': citation.get('year', year),
                    'doi': doi,
                    'authors': citation.get('authors', ''),
                    'first_author_family': citation.get('first_author_family', ''),
                    'first_author_given': citation.get('first_author_given', ''),
                    'author_count': citation.get('author_count', 0),
                    'abstract': citation.get('abstract', ''),
                    'page': citation.get('page', ''),
                    'reference_count': citation.get('reference_count', 0),
                    'cited_by_count': citation.get('cited_by_count', 0),
                    'citation_apa': citation.get('citation_apa', ''),
                    'url': citation.get('url', ''),
                    'pdf_url': citation.get('pdf_url', ''),
                }
            else:
                # API è¯·æ±‚å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯
                result = {
                    'art_id': artid,
                    'title': art.get('title_web', ''),
                    'volume': vol,
                    'number': num,
                    'year': year,
                    'doi': doi,
                    'authors': '',
                    'first_author_family': '',
                    'first_author_given': '',
                    'author_count': 0,
                    'abstract': '',
                    'page': '',
                    'reference_count': 0,
                    'cited_by_count': 0,
                    'citation_apa': '',
                    'url': f'https://doi.org/{doi}',
                    'pdf_url': '',
                }
        
        if (index + 1) % 50 == 0:
            logger.info(f"Progress: {index + 1}/{len(articles)} articles processed")
        
        return result
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ï¼ˆé™ä½å¹¶å‘æ•°ä»¥é¿å…é€Ÿç‡é™åˆ¶ï¼‰
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(process_article, art, i): i 
            for i, art in enumerate(articles)
        }
        
        for future in as_completed(futures):
            try:
                result = future.result()
                detailed_articles.append(result)
            except Exception as e:
                idx = futures[future]
                logger.error(f"Error processing article {idx}: {e}")
    
    # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æ•°æ®åº“
    logger.info("\nStep 3: Saving to database files...")
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame(detailed_articles)
    
    # æ•°æ®æ¸…æ´—å’Œæ’åº
    for col in ['year', 'volume', 'number', 'author_count', 'reference_count', 'cited_by_count']:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
    
    # å¡«å……ç©ºå­—ç¬¦ä¸²
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].fillna('')
    
    # æŒ‰å¹´ä»½ã€å·å·ã€æœŸå·æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œå¦åˆ™æŒ‰art_idæ’åºï¼‰
    df = df.sort_values(['year', 'volume', 'number'], ascending=[True, True, True])
    df = df.reset_index(drop=True)
    
    # è°ƒæ•´åˆ—é¡ºåºï¼Œå°†art_idæ”¾åœ¨å‰é¢
    cols = ['art_id'] + [col for col in df.columns if col != 'art_id']
    df = df[cols]
    
    # ä¿å­˜ä¸»æ•°æ®åº“æ–‡ä»¶
    output_path = Path(__file__).parent / "findsj.dta"
    df.to_stata(str(output_path), write_index=False, version=118)
    logger.info(f"âœ… Main database saved: {output_path}")
    
    # ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯æ–‡ä»¶
    version_df = pd.DataFrame([{
        'update_date': datetime.now().strftime('%Y-%m-%d'),
        'update_time': datetime.now().strftime('%H:%M:%S'),
        'total_articles': len(df),
        'articles_with_doi': (df['doi'] != '').sum(),
        'articles_with_citation': (df['citation_apa'] != '').sum(),
        'year_min': int(df['year'].min()),
        'year_max': int(df['year'].max()),
    }])
    version_path = Path(__file__).parent / "findsj_version.dta"
    version_df.to_stata(str(version_path), write_index=False, version=118)
    logger.info(f"âœ… Version info saved: {version_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "=" * 60)
    logger.info("Database Update Summary")
    logger.info("=" * 60)
    logger.info(f"Total articles: {len(df)}")
    logger.info(f"Year range: {int(df['year'].min())} - {int(df['year'].max())}")
    logger.info(f"Articles with DOI: {(df['doi'] != '').sum()}")
    logger.info(f"Articles with authors: {(df['authors'] != '').sum()}")
    logger.info(f"Articles with abstract: {(df['abstract'] != '').sum()}")
    logger.info(f"Average citations: {df['cited_by_count'].mean():.1f}")
    logger.info(f"Total citations: {df['cited_by_count'].sum()}")
    logger.info("=" * 60)
    
    # ä¿å­˜è¯¦ç»†æ—¥å¿—
    log_path = Path(__file__).parent / "update_log.json"
    log_data = {
        'update_datetime': datetime.now().isoformat(),
        'total_articles': len(df),
        'articles_with_doi': int((df['doi'] != '').sum()),
        'articles_with_citation': int((df['citation_apa'] != '').sum()),
        'year_range': [int(df['year'].min()), int(df['year'].max())],
        'top_cited': df.nlargest(10, 'cited_by_count')[['title', 'cited_by_count', 'year']].to_dict('records')
    }
    with open(log_path, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, indent=2, ensure_ascii=False)
    logger.info(f"ğŸ“‹ Update log saved: {log_path}")
    
    logger.info("\nâœ… Database update completed successfully!")

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    try:
        update_database()
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    main()
